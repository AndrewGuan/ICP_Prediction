{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(filepath, filetype):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(filetype.lower()):\n",
    "                count = count + 1\n",
    "    return(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_patient_information(sn_num):\n",
    "    samples_per_hour = 1080000 # sample rate is 300Hz\n",
    "    samples_per_min = int(samples_per_hour / 60)\n",
    "    \n",
    "    # parse the large data by one hour per file\n",
    "    interval = samples_per_hour\n",
    "    offset = 0\n",
    "    count = 1\n",
    "\n",
    "    ascii_grid = np.loadtxt(\"SN_\" + sn_num + \".asc\", skiprows=2)\n",
    "    while offset + interval < len(ascii_grid) :\n",
    "        output_data = []\n",
    "        for i in range(interval):\n",
    "            output_data.append(float(\"{0:.3f}\".format(ascii_grid[i + offset][1])))    \n",
    "        output_name = 'data_' + sn_num + '/' + str(count) + '_data.csv'\n",
    "        savetxt(output_name, output_data, delimiter=',')\n",
    "        offset = offset + interval\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_one_min_for_each_patient(sn_num):\n",
    "    samples_per_hour = 1080000 # sample rate is 300Hz\n",
    "    samples_per_min = int(samples_per_hour / 60)\n",
    "        \n",
    "    # get the average data of one miniute for a sn\n",
    "    dir_name = \"data_\" + sn_num\n",
    "\n",
    "    file_count = list_files(dir_name, '.csv')\n",
    "    output_data = []\n",
    "    for i in range(file_count):\n",
    "        data = pd.read_csv('data_' + sn_num + '/' + str(i + 1) + '_data.csv', header = None)\n",
    "        offset = 0\n",
    "        X = data.iloc[:,0]  #independent columns\n",
    "        while offset + samples_per_min <= len(X):\n",
    "            output_data.append(float(\"{0:.2f}\".format(np.mean(data.iloc[offset: offset + samples_per_min, 0]))))\n",
    "            offset = offset + samples_per_min\n",
    "    output_name = 'SN_' + sn_num + '_per_min.csv'\n",
    "    savetxt(output_name, output_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_patient_information(\"34\") # \"40\", \"41\", \"42\", \"43\"\n",
    "# avg_one_min_for_each_patient(\"34\") # \"40\", \"41\", \"42\", \"43\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the points \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(10, 4), dpi=120, facecolor='w', edgecolor='k')\n",
    "\n",
    "sn_num = \"43\"\n",
    "data = pd.read_csv('SN_' + sn_num + '_per_min.csv', header = None)\n",
    "x = []\n",
    "for i in range(len(data)):\n",
    "    x.append((i + 1)/60) \n",
    "plt.plot(x, data) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test\n",
    "#sn_num = \"34\"\n",
    "#data = pd.read_csv('SN_' + sn_num + '_per_min.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ICP(num):\n",
    "    if num > 13:\n",
    "        return 1\n",
    "    elif num < 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(data)\n",
    "num_train = int(num * 0.8) # 0.8 usually\n",
    "num_test = num - num_train\n",
    "samples_four_hours = 60 * 4\n",
    "interval = 30\n",
    "offset = 0\n",
    "index_train = 0\n",
    "index_test = num_train\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "# 0 + 240 + 30 = 270\n",
    "while index_train + samples_four_hours + interval < num_train:\n",
    "    x_data = []\n",
    "    for i in range(samples_four_hours):\n",
    "        x_data.append(data[0][index_train + i])\n",
    "    x_train.append(x_data)\n",
    "    y_train.append(check_ICP(data[0][index_train + samples_four_hours + interval]))\n",
    "    #y_train.append(1)\n",
    "    index_train = index_train + 1\n",
    "\n",
    "while index_test + samples_four_hours + interval < num:\n",
    "    x_data = []\n",
    "    for i in range(samples_four_hours):\n",
    "        x_data.append(data[0][index_test + i])\n",
    "    x_test.append(x_data)\n",
    "    y_test.append(check_ICP(data[0][index_test + samples_four_hours + interval]))\n",
    "    index_test = index_test + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Flatten, Activation, add\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def build_model(SHAPE, nb_classes, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    input_layer = Input(shape=SHAPE)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv1D(32, 3, activation='relu')(input_layer)\n",
    "\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv1D(48, 3, activation='relu')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv1D(64, 3, activation='relu')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Step 1\n",
    "    x = Conv1D(96, 3, activation='relu')(x)\n",
    "    # Step 2 - Pooling\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    # Step 3 - Flattening\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "\n",
    "    x = Dense(output_dim=256, activation='relu')(x)\n",
    "    # Dropout\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(output_dim=2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SHAPE = (samples_four_hours, 1)\n",
    "model = build_model(SHAPE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(lr=1.0e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "X = np.array(x_train)\n",
    "Y = np.array(y_train)\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "model.fit(X, Y, batch_size=50, epochs=100)\n",
    "\n",
    "# make prediction\n",
    "X_test = np.array(x_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "\n",
    "#Y_test = np.array(y_train)\n",
    "Y_test = np.array(y_test)\n",
    "from keras.utils import to_categorical\n",
    "Y_test = to_categorical(Y_test)\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "report = classification_report(Y_test, y_pred)\n",
    "tn = cm[0][0]\n",
    "fn = cm[1][0]\n",
    "tp = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "if tp == 0:\n",
    "    tp = 1\n",
    "if tn == 0:\n",
    "    tn = 1\n",
    "if fp == 0:\n",
    "    fp = 1\n",
    "if fn == 0:\n",
    "    fn = 1\n",
    "TPR = float(tp)/(float(tp)+float(fn))\n",
    "FPR = float(fp)/(float(fp)+float(tn))\n",
    "accuracy = round((float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
